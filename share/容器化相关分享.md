# 主题
* fs -> unionFs -> docker:Image & Container本质
* 为什么要容器化 & 容器化之后的开发注意点
* Docker网络基础 
* kubernetes简介 & kubernetes主流网络解决方案
* 应用发布的六种策略 & 公司未来的发布模式
* legends 针对未来发布的一些改造

# FS -> Union mount -> UnionFS
## Union mount 
联合挂载是指把多个不同的目录(directory)，合并成一个。

文件系统A  
![1](ref/unionfsdemo/1.png)

文件系统B  
![2](ref/unionfsdemo/2.png)

A && B  
![3](ref/unionfsdemo/3.png)

Union mount 是接口层，定义了 Union mount 需要实现的功能，具体的实现比如 linux 早期的 union mount，plan9 等。

## UnionFS (Union File System)

Linux： Inheriting File System  -> UnionFS  
允许多个单独的文件系统的目录，透明的**覆盖**在一起，形成一个联合的文件系统。在合并之后的目录之中，能够看到所有子文件系统的内容。

wikipedia : <https://en.wikipedia.org/wiki/UnionFS>

### UnionFS的一些问题
* 同名文件  
    
    在合并子文件系统的时候，多个文件系统之间，存在**优先级**，所以，如果多个文件系统在同一个目录下，存在一个相同的文件名，则在合并之后的文件系统里面，看到的是，优先级高的那个文件系统的文件。 
    
* 如何创建/删除文件  

    由于合并的子文件系统，可能是Read-Only / Read-Write 的文件系统，所以为了对虚拟合并副本的写入，会将这一类操作定向到一个**新的可写文件系统**。对这个可写文件系统的操作，不会去修改底层的子文件系统，这里一般采用 Copy-on-Write 技术（只有真正发生写操作的时候，才会去创建这一可写层）。

# Docker
![Docker](ref/docker_architech.jpg)

## Docker Image
DockerImage 是基于 UnionFS 技术整合的多层文件系统（镜像分层技术）  
**aufs** (_Advanced multi-layered unification filesystem_)   
**overlay2**

    FROM centos:latest
    
    ENV JAVA_HOME=/usr/local/jdk1.8.0_201
    ENV TOMCAT_HOME=/home/tomcat/apache-tomcat-8.5.32
    ENV PATH=$JAVA_HOME/bin:$TOMCAT_HOME/bin:$PATH
    
    RUN useradd -d /home/tomcat -s /usr/sbin/nologin tomcat \
        && gpasswd -a tomcat tomcat \
        && chown -R tomcat:tomcat /home/tomcat
    
    ADD jdk-8u201-linux-x64.tar.gz /usr/local
    ADD apache-tomcat-8.5.32.tar.gz /home/tomcat
    
    ENTRYPOINT ["catalina.sh","run"]
    
    
#### 理解 Docker 镜像分层技术的案例：根据已有的 jdk 安装包，在 DockerImage 中安装 jdk
方式一

    ADD jdk-8u201-linux-x64.tar.gz /usr/local

方式二

    COPY jdk-8u201-linux-x64.tar.gz /usr/local
    RUN cd /usr/local \
        && tar -xvf jdk-8u201-linux-x64.tar.gz \
        && rm jdk-8u201-linux-x64.tar.gz

## Docker Container
宿主机上的一个进程

# 容器化
## KVM技术在发布时有哪些问题
* 测试环境跟生产环境不一致带来的问题
* 机器资源利用率低
    
        KVM 1:10
        KVM本身的资源消耗
* 预发环境的资源浪费        
* 发布流程复杂


## 容器化解决了哪些问题
* 一份镜像随处发布
* 更高的机器利用率，节约成本 
    CPU 资源的利用率提升
* 更便捷智能发布方式
* ReplicationController (RC RS)
* HPA(Horizontal Pod Autoscaler)
    CPUUtilizationPercentage
    自定义指标 QPS TPS

# 系统开发需要注意哪些问题?
* 有状态的应用
* IP分配策略
* 是否需要暴露接口给外部使用

## 容器网络
### Docker 的网络实现
##### Network Namespace (网络命名空间)
Linux 引入网络命名空间，不同命名空间的网络栈是完全隔离的，彼此之间无法通信
Docker也正是利用了网络的命名空间特性，实现了不同容器之间网络的隔离

![NetNamespace](ref/net/netnamespace.png)

##### Veth Peer
Veth设备对是为了在不同的网络命名空间之间进行通信，利用它可以直接将两个网络命名空间连接起来
![Veth](ref/net/veth.png)

##### bridge (虚拟网桥)
多网卡之间数据交换的二层设备（数据链路层）

![docker_net](ref/net/docker_net.png)

##### 如何打通宿主机与容器之间的网络
 在bridge模式下，Docker Daemon第1次启动时会创建一个虚拟的网桥，默认的名字是docker0，并在私有网络命名空间中给这个网桥分配一个子网。针对由Docker创建出来的每一个容器，都会创建一个虚拟的以太网设备（**Veth Peer**），其中一端关联到网桥上，另一端使用Linux的网络命名空间技术，映射到容器内的eth0设备，然后从网桥的地址段内给eth0接口分配一个IP地址。

##### 宿主机eth0网卡与docker0网桥? 
iptables/Netfilter  
Linux提供了一套机制来为用户实现自定义的数据包处理过程。

在Linux网络协议栈中有一组回调函数挂接点，通过这些挂接点挂接的钩子函数可以在Linux网络栈处理数据包的过程中对数据包进行一些操作，例如过滤、修改、丢弃等。整个挂接点技术叫作Netfilter和Iptables。

Netfilter负责在内核中执行各种挂接的规则，运行在内核模式中；而Iptables是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表。通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制。

### Docker 网络的局限性
Docker没有考虑到多主机互联的网络解决方案   

# 容器编排
## kubernetes(k8s)
![kubernetes](../k8s/ref/k8s_03.jpg)

## Pods
#### pods 网络模型
#### pause 容器的作用
* 作为当前pod内，所有container的网络/挂载的管理容器
* 作为pod状态的代表

## minikube
[mac 本地 minikube 环境搭建](https://www.cnblogs.com/cocowool/p/minikube_setup_and_first_sample.html)
 
### 容器IP如何分配？
* Container -> CNI -> IPAM  
[host-local](https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local)  
[Dynamic Host Configuration Protocol，DHCP](https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp)
* KVM  分配的IP直接写死在虚拟机的配置文件里面

其他参考文档：  
[Some standard networking plugins, maintained by the CNI team](https://github.com/containernetworking/plugins)

# kubernetes网络解决方案
## 直接路由方案
![kubernetes_network_route_direct](ref/kubernetes_network_routedirect.png)

## flannel 网络叠加
![kubernetes_network](ref/kubernetes_network.png)

## Open vSwitch
![kubernetes_network_openswitch](ref/kubernetes_network_openvswitch.png)


# 应用发布的六种策略
原文参考: [Six Strategies for Application Deployment](https://thenewstack.io/deployment-strategies)

## Recreated 重建
完全停掉版本A，发布版本B
![recreate](ref/recreate.gif)

#### 优点
* 操作简单

#### 缺点
* 需要停服，用户体验太差

## Ramped 灰度发布(也叫滚动升级 or 增量发布)
对集群中的多台机器，分批发布，直到所有机器容器全部完成升级  
![ramped](ref/ramped.gif)

#### 优点
* 操作简单 (容器化之后操作简单)
* 对于有状态的应用，能够友好的保持应用状态。

#### 缺点
* 发布与回滚的时间较长
* 对于请求流量不可控
* 对于一些 http 接口，很难做到向下版本兼容

## Blue/Green 蓝绿发布
操作步骤：
1. 同时部署两个完全相互隔离的集群A，集群B，
2. 集群B从对外流量中剥离之后，升级到新版本
3. 测试无误之后，将请求流量切换到集群B
4. 升级集群A
5. A升级完之后，在平均分配流量到AB集群  
![blue_green](ref/blue_green.gif)


#### 优点
* 瞬间升级/瞬间回滚（对于请求来说）
* 避免版本升级时的兼容性问题，相互依赖问题等
#### 缺点
* 需要双份的机器资源
* 需要在投入生产之前，对整个平台进行适当的测试
* 很难处理一些有状态的服务（Stateful application）

## Canary 金丝雀
版本B与版本A同时对外提供服务，一开始A承担90%的流量 B承担10%的流量，再B确认无误之后，再主键扩大B的比例，直到100%。  
流量分配策略一般基于**权重**实现。  
![canary](ref/canary.gif)

#### 优点
* 能够实现部分用户升级
* 对于控制异常比例，性能监控 较为方便
* 快速回滚
#### 缺点
* 回滚较慢
* 难以处理一些连续的有逻辑顺序的HTTP请求

## A/B testing
根据请求条件，将用户请求路由到指定的集群中。不是基于硬件网络的实现，需要软件代码的支持，针对不同的业务情况，实现起来也较为复杂
常见的方式：白名单/客户端版本号等  
![a-b](ref/a-b.gif)

#### 优点
* 多版本并行运行
* 流量控制方便
#### 缺点
* 需要更智能的路由策略 
* 对于指定 session 的异常定位，分布式链路追踪等功能变得难以处理

## Shadow 影子
将版本A的请求，转发一份给版本B，版本B只接受请求，不做任何响应  
![shadow](ref/shadow.gif)

#### 优点
* 便于做新功能的压力测试
* 对用户无影响
#### 缺点
* 两份机器资源消耗
* 实现起来较为复杂
* 不适用于一些非查询类的接口，比如扣款等

## 不同部署模式的对比
![deployment_strategies](ref/k8s_deployment_strategies.png)

#### 下一版发布模式（也可能是其他方案，还没有确定）
#### Legends针对未来发布模式的一些定制化改造




